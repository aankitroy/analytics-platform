# docker-compose.yaml - Connects to External PG/Redis

# Networks are not strictly necessary for connecting to external hosts
# via the default bridge network, but can be useful if you add other
# local containers later. Let's remove it for simplicity.
# networks:
#   airflow-network:
#     driver: bridge

volumes:
  airflow-dags:
  airflow-logs:
  airflow-config:
  airflow-data:

x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE}
  environment:
    # --- Airflow Environment Variables ---
    AIRFLOW_HOME: ${AIRFLOW_HOME}
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${EXTERNAL_POSTGRES_HOST}:${EXTERNAL_POSTGRES_PORT}/${POSTGRES_DB}
    AIRFLOW__CELERY__BROKER_URL: redis://${EXTERNAL_REDIS_USER}:${REDIS_PASSWORD}@${EXTERNAL_REDIS_HOST}:${EXTERNAL_REDIS_PORT}/${REDIS_DB_BROKER}
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${EXTERNAL_POSTGRES_HOST}:${EXTERNAL_POSTGRES_PORT}/${POSTGRES_DB}
    AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'True'
    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'False'
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY}
  volumes:
    - ./dags:${AIRFLOW_HOME}/dags
    - ./logs:${AIRFLOW_HOME}/logs
    - ./config/airflow.cfg:${AIRFLOW_HOME}/airflow.cfg:ro
    - airflow-data:${AIRFLOW_HOME}/data
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
    interval: 30s
    timeout: 10s
    retries: 5
    start_period: 30s

services:
  # Airflow Database Initialization Service
  # This service runs commands to initialize the Airflow DB and create a user
  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: >
      bash -c '
      echo "Waiting for database to be ready..." &&
      while ! airflow db check; do
        echo "Database not ready yet. Waiting..."
        sleep 5
      done &&
      echo "Database is ready!" &&
      airflow db migrate &&
      airflow db init &&
      airflow users create -u admin -f Aankit -l Roy -r Admin -e aankitroy1998@gmail.com -p admin
      '
    environment:
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${EXTERNAL_POSTGRES_HOST}:${EXTERNAL_POSTGRES_PORT}/${POSTGRES_DB}
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_PASSWORD: 'admin'
      _AIRFLOW_WWW_USER_EMAIL: 'aankitroy1998@gmail.com'
    restart: "no"
    healthcheck:
      test: ["CMD-SHELL", "airflow db check || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Airflow Webserver Service
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    ports:
      - "8080:8080"
    command: >
      bash -c '
      airflow db init &&
      airflow db check &&
      airflow webserver
      '
    environment:
      AIRFLOW__WEBSERVER__WORKERS: "2"
      AIRFLOW__WEBSERVER__WORKER_CLASS: "sync"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Airflow Scheduler Service
  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: airflow scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname $${HOSTNAME} || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Airflow Worker Service (Celery Worker)
  airflow-worker:
    <<: *airflow-common
    container_name: airflow_worker
    user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
    command: airflow celery worker
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "airflow celery worker --check || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Optional: Celery Flower for monitoring workers
  # airflow-flower:
  #   image: ${AIRFLOW_IMAGE}
  #   container_name: airflow_flower
  #   user: "${AIRFLOW_UID}:${AIRFLOW_GID}"
  #   command: airflow celery flower --address=0.0.0.0 --port=5555 # Bind to all interfaces
  #   environment:
  #     AIRFLOW_HOME: ${AIRFLOW_HOME}
  #     AIRFLOW__CELERY__BROKER_URL: redis://:${REDIS_PASSWORD}@${EXTERNAL_REDIS_HOST}:${EXTERNAL_REDIS_PORT}/${REDIS_DB_BROKER}
  #     # Flower typically only needs the broker URL, but providing result backend is fine
  #     # AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${EXTERNAL_POSTGRES_HOST}:${EXTERNAL_POSTGRES_PORT}/${POSTGRES_DB}
  #   ports:
  #     - "5555:5555" # Default Flower port
  #   depends_on:
  #     - airflow-worker # Flower should start after workers/broker are up
  #   restart: unless-stopped